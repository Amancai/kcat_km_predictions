{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "import warnings\n",
    "import json\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy import stats\n",
    "import xgboost as xgb\n",
    "from hyperopt import fmin, rand, hp, Trials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script to train and validate the gradient boosting models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2375, 561)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_pickle(join(\"partitions\", \"train_df.pkl\"))\n",
    "data_test = pd.read_pickle(join(\"partitions\", \"test_df.pkl\"))\n",
    "\n",
    "data_train = data_train.rename(columns={\"kcat_km\": \"log10_kcat_km\"})\n",
    "data_test = data_test.rename(columns={\"kcat_km\": \"log10_kcat_km\"})\n",
    "\n",
    "len(data_train), len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = list(pd.read_pickle(join(\"partitions\", \"CV_train_indices.pkl\")))\n",
    "test_indices = list(pd.read_pickle(join(\"partitions\", \"CV_test_indices.pkl\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model with only enzyme representations (ESM-1b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create input matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ESM1b = np.array(list(data_train[\"ESM1b\"]))\n",
    "train_X = train_ESM1b\n",
    "train_Y = np.array(list(data_train[\"log10_kcat_km\"]))\n",
    "\n",
    "test_ESM1b = np.array(list(data_test[\"ESM1b\"]))\n",
    "test_X = test_ESM1b\n",
    "test_Y = np.array(list(data_test[\"log10_kcat_km\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_mse_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    del param[\"num_rounds\"]\n",
    "    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "    param[\"tree_method\"] = \"gpu_hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "    MSE = []\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "        dvalid = xgb.DMatrix(train_X[test_index])\n",
    "        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "        y_valid_pred = bst.predict(dvalid)\n",
    "        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "\n",
    "space_gradient_boosting = {\n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 1),\n",
    "    \"max_depth\": hp.uniform(\"max_depth\", 4,12),\n",
    "    #\"subsample\": hp.uniform(\"subsample\", 0.7, 1),\n",
    "    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 5),\n",
    "    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 5),\n",
    "    \"max_delta_step\": hp.uniform(\"max_delta_step\", 0, 5),\n",
    "    \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.1, 15),\n",
    "    \"num_rounds\":  hp.uniform(\"num_rounds\", 20, 200)}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 200, trials=trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and validating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = best \n",
    "\n",
    "with open(join(\"/gpfs/project/dit55hov/best_param_ESM1b.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(best, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "\n",
    "np.save(join(\"training_results\", \"Pearson_CV_xgboost_ESM1b.npy\"), np.array(Pearson))\n",
    "np.save(join(\"training_results\", \"MSE_CV_xgboost_ESM1b.npy\"), np.array(MSE))\n",
    "np.save(join(\"training_results\", \"R2_CV_xgboost_ESM1b.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))\n",
    "\n",
    "np.save(join(\"training_results\", \"y_test_pred_xgboost_ESM1b.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"training_results\", \"y_test_true_xgboost_ESM1b.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model with reaction fingerprints (rxnfp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create input matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rxnfp = np.array(list(data_train[\"rxnfp\"]))\n",
    "train_X = train_rxnfp\n",
    "train_Y = np.array(list(data_train[\"log10_kcat_km\"]))\n",
    "\n",
    "test_rxnfp = np.array(list(data_test[\"rxnfp\"]))\n",
    "test_X = test_rxnfp\n",
    "test_Y = np.array(list(data_test[\"log10_kcat_km\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 200, trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = best \n",
    "\n",
    "with open(join(\"/gpfs/project/dit55hov/best_param_rxnfp.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(best, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "\n",
    "np.save(join(\"training_results\", \"Pearson_CV_xgboost_rxnfp.npy\"), np.array(Pearson))\n",
    "np.save(join(\"training_results\", \"MSE_CV_xgboost_rxnfp.npy\"), np.array(MSE))\n",
    "np.save(join(\"training_results\", \"R2_CV_xgboost_rxnfp.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))\n",
    "\n",
    "np.save(join(\"training_results\", \"y_test_pred_xgboost_rxnfp.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"training_results\", \"y_test_true_xgboost_rxnfp.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model with enzyme representations (ESM-1b) + reaction fingerprints (rxnfp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create input matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rxnfp = np.array(list(data_train[\"esm1b_rxnfp\"]))\n",
    "train_X = train_rxnfp\n",
    "train_Y = np.array(list(data_train[\"log10_kcat_km\"]))\n",
    "\n",
    "test_rxnfp = np.array(list(data_test[\"esm1b_rxnfp\"]))\n",
    "test_X = test_rxnfp\n",
    "test_Y = np.array(list(data_test[\"log10_kcat_km\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 200, trials=trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and validating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = best\n",
    "\n",
    "with open(join(\"/gpfs/project/dit55hov/best_param_esm1b_rxnfp.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(best, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "\n",
    "np.save(join(\"training_results\", \"Pearson_CV_xgboost_esm1b_rxnfp.npy\"), np.array(Pearson))\n",
    "np.save(join(\"training_results\", \"MSE_CV_xgboost_esm1b_rxnfp.npy\"), np.array(MSE))\n",
    "np.save(join(\"training_results\", \"R2_CV_xgboost_esm1b_rxnfp.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))\n",
    "\n",
    "np.save(join(\"training_results\", \"y_test_pred_xgboost_esm1b_rxnfp.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"training_results\", \"y_test_true_xgboost_esm1b_rxnfp.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model with only SMILES substrate representations (ChemBERTa2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create input matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ChemBERTa2 = np.array(list(data_train[\"ChemBERTa2\"]))\n",
    "train_X = train_ChemBERTa2\n",
    "train_Y = np.array(list(data_train[\"log10_kcat_km\"]))\n",
    "\n",
    "test_ChemBERTa2 = np.array(list(data_test[\"ChemBERTa2\"]))\n",
    "test_X = test_ChemBERTa2\n",
    "test_Y = np.array(list(data_test[\"log10_kcat_km\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 200, trials=trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and validating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = best \n",
    "\n",
    "with open(join(\"/gpfs/project/dit55hov/best_param_ESM1b.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(best, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "\n",
    "np.save(join(\"training_results\", \"Pearson_CV_xgboost_ChemBERTa2.npy\"), np.array(Pearson))\n",
    "np.save(join(\"training_results\", \"MSE_CV_xgboost_ChemBERTa2.npy\"), np.array(MSE))\n",
    "np.save(join(\"training_results\", \"R2_CV_xgboost_ChemBERTa2.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))\n",
    "\n",
    "np.save(join(\"training_results\", \"y_test_pred_xgboost_ChemBERTa2.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"training_results\", \"y_test_true_xgboost_ChemBERTa2.npy\"), test_Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
